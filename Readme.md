# üöÄ Projet Big Data : Spark, Hadoop, PostgreSQL, Jupyter avec Docker

## 1. Cloner le d√©p√¥t

La premi√®re √©tape consiste √† cloner le projet depuis GitHub.  
Ouvre un terminal et ex√©cute la commande suivante‚ÄØ:

```bash
git clone https://github.com/Jaouad-Said/spark-docker.git
cd spark-docker
```

## 2. D√©marrer l'environnement Big Data

Apr√®s avoir clon√© le d√©p√¥t et acc√©d√© au dossier du projet, lance tous les services n√©cessaires avec Docker Compose‚ÄØ:

```bash
docker compose up -d
```

Pour v√©rifier que tous les services sont bien d√©marr√©s, utilise‚ÄØ:

```bash
docker ps
```

Les interfaces web des diff√©rents services sont accessibles sur‚ÄØ:

Jupyter Notebook : http://localhost:8888

Spark Master UI : http://localhost:8080

HDFS UI : http://localhost:9870

## 3. Cr√©ation et v√©rification du dossier HDFS

```bash
docker exec -it hadoop-namenode bash

# Dans le conteneur du namenode :
hdfs dfs -mkdir -p /user/pros
hdfs dfs -ls /user
```

## 4. Acc√®s √† Jupyter Notebook

Pour obtenir le lien d'acc√®s √† Jupyter Notebook lanc√© dans le conteneur, ex√©cute :

```bash
docker-compose exec jupyter jupyter server list

Currently running servers:
http://f0f9477f3b6f:8888/?token=<TON_TOKEN> :: /home/jovyan
```

Ouvre le lien indiqu√© dans ton navigateur pour acc√©der √† l‚Äôinterface Jupyter en utilisant TOKEN.

Dans l‚Äôinterface Jupyter ou depuis VS Code, ouvre le notebook voulu (par exemple operations.ipynb) Lance les cellules du notebook pour r√©aliser les op√©rations Spark, comme l‚Äôimportation des donn√©es depuis PostgreSQL et l‚Äôaffichage du DataFrame

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("check").getOrCreate()

df = spark.read.jdbc(
    url="jdbc:postgresql://postgres:5432/demo",
    table="products",
    properties={
        "user": "postgres",
        "password": "postgres",
        "driver": "org.postgresql.Driver"
    }
)

df.select("id", "name", "brand", "price", "stock_quantity").show()
```

## Export des donn√©es de Spark vers HDFS et v√©rification

## 1. √âcriture depuis Spark vers HDFS

Apr√®s avoir transform√© tes donn√©es dans Spark, tu peux exporter les r√©sultats au format CSV dans HDFS (via le port 8020)‚ÄØ:

```python
df.select("id", "name", "brand", "price", "stock_quantity") \
  .write.csv("hdfs://hadoop-namenode:8020/user/pros/products_export", header=True, mode="overwrite")
```

## üí° Astuce D√©pannage : Erreur de taille maximale RPC entre Spark et Hadoop

Si tu rencontres cette erreur lors d‚Äôun export ou d‚Äôun import de fichiers volumineux dans HDFS‚ÄØ:

Ajoute ce param√®tre lors de la cr√©ation de la SparkSession‚ÄØ:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("products_job") \
    .config("spark.hadoop.ipc.maximum.data.length", 536870912) \
    .getOrCreate()
```

## Lancement d'un job Spark (spark-submit)

Le script Python du job Spark √† ex√©cuter est d√©j√† pr√©sent dans le r√©pertoire `jobs/` du projet‚ÄØ:
Pour lancer ce job depuis le conteneur `spark-master`, utilise les commandes suivantes‚ÄØ:

```bash
docker exec -u 0 -it spark-master bash

# (Dans le conteneur)
export HOME=/tmp USER=root

spark-submit --master spark://spark-master:7077 \
  --jars /opt/spark/jars/postgresql-42.7.5.jar \
  /opt/spark/jobs/products_job.py
```
