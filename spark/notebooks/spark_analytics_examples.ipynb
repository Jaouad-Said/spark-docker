{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e0873a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Spark + PostgreSQL Analytics Examples\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Save this as: spark/notebooks/spark_analytics_examples.ipynb\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ═══════════════════════════════════════════════════════════════════════════════\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# SETUP & CONNECTION\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# ═══════════════════════════════════════════════════════════════════════════════\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Spark + PostgreSQL Analytics Examples\n",
    "# Save this as: spark/notebooks/spark_analytics_examples.ipynb\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SETUP & CONNECTION\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark with PostgreSQL connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-Commerce Analytics\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.7.5.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# PostgreSQL connection properties\n",
    "pg_properties = {\n",
    "    \"user\": \"sparkuser\",\n",
    "    \"password\": \"sparkpass\", \n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"url\": \"jdbc:postgresql://postgres:5432/demo\"\n",
    "}\n",
    "\n",
    "print(\"✅ Spark session created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# DATA LOADING\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Load main tables into Spark DataFrames\n",
    "customers_df = spark.read.jdbc(**pg_properties, table=\"customers\")\n",
    "orders_df = spark.read.jdbc(**pg_properties, table=\"orders\")\n",
    "order_items_df = spark.read.jdbc(**pg_properties, table=\"order_items\")\n",
    "products_df = spark.read.jdbc(**pg_properties, table=\"products\")\n",
    "categories_df = spark.read.jdbc(**pg_properties, table=\"categories\")\n",
    "reviews_df = spark.read.jdbc(**pg_properties, table=\"reviews\")\n",
    "\n",
    "print(\"📊 Data loaded successfully!\")\n",
    "print(f\"Customers: {customers_df.count():,}\")\n",
    "print(f\"Orders: {orders_df.count():,}\")\n",
    "print(f\"Order Items: {order_items_df.count():,}\")\n",
    "print(f\"Products: {products_df.count():,}\")\n",
    "print(f\"Reviews: {reviews_df.count():,}\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# EXAMPLE 1: CUSTOMER ANALYTICS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 1: CUSTOMER LIFETIME VALUE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Customer LTV analysis with Spark SQL\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "customer_ltv = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_tier,\n",
    "        COUNT(DISTINCT c.id) as customer_count,\n",
    "        AVG(c.total_lifetime_value) as avg_ltv,\n",
    "        SUM(c.total_lifetime_value) as total_ltv,\n",
    "        AVG(DATEDIFF(CURRENT_DATE(), c.registration_date)) as avg_days_since_registration\n",
    "    FROM customers c\n",
    "    WHERE c.total_lifetime_value > 0\n",
    "    GROUP BY c.customer_tier\n",
    "    ORDER BY avg_ltv DESC\n",
    "\"\"\")\n",
    "\n",
    "customer_ltv.show()\n",
    "\n",
    "# Convert to pandas for visualization\n",
    "ltv_pandas = customer_ltv.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(ltv_pandas['customer_tier'], ltv_pandas['avg_ltv'])\n",
    "plt.title('Average LTV by Customer Tier')\n",
    "plt.ylabel('Average LTV ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(ltv_pandas['customer_count'], labels=ltv_pandas['customer_tier'], autopct='%1.1f%%')\n",
    "plt.title('Customer Distribution by Tier')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# EXAMPLE 2: SALES TREND ANALYSIS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 2: MONTHLY SALES TREND ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Monthly sales trends\n",
    "monthly_sales = orders_df.filter(col(\"payment_status\") == \"paid\") \\\n",
    "    .withColumn(\"year_month\", date_format(col(\"order_date\"), \"yyyy-MM\")) \\\n",
    "    .groupBy(\"year_month\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_orders\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "    ) \\\n",
    "    .orderBy(\"year_month\")\n",
    "\n",
    "monthly_sales.show(20)\n",
    "\n",
    "# Plot sales trends\n",
    "sales_pandas = monthly_sales.toPandas()\n",
    "sales_pandas['year_month'] = pd.to_datetime(sales_pandas['year_month'])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(sales_pandas['year_month'], sales_pandas['total_revenue'])\n",
    "plt.title('Monthly Revenue Trend')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(sales_pandas['year_month'], sales_pandas['total_orders'])\n",
    "plt.title('Monthly Order Volume')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(sales_pandas['year_month'], sales_pandas['avg_order_value'])\n",
    "plt.title('Average Order Value Trend')\n",
    "plt.ylabel('AOV ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(sales_pandas['year_month'], sales_pandas['unique_customers'])\n",
    "plt.title('Monthly Unique Customers')\n",
    "plt.ylabel('Unique Customers')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# EXAMPLE 3: PRODUCT PERFORMANCE ANALYSIS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 3: TOP PERFORMING PRODUCTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create temp views for complex analysis\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "order_items_df.createOrReplaceTempView(\"order_items\")\n",
    "categories_df.createOrReplaceTempView(\"categories\")\n",
    "reviews_df.createOrReplaceTempView(\"reviews\")\n",
    "\n",
    "# Top products by revenue with category information\n",
    "top_products = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.name as product_name,\n",
    "        c.name as category_name,\n",
    "        p.brand,\n",
    "        p.price,\n",
    "        COUNT(oi.id) as total_orders,\n",
    "        SUM(oi.quantity) as units_sold,\n",
    "        SUM(oi.line_total) as total_revenue,\n",
    "        AVG(r.rating) as avg_rating,\n",
    "        COUNT(r.id) as review_count\n",
    "    FROM products p\n",
    "    LEFT JOIN categories c ON p.category_id = c.id\n",
    "    LEFT JOIN order_items oi ON p.id = oi.product_id\n",
    "    LEFT JOIN orders o ON oi.order_id = o.id AND o.payment_status = 'paid'\n",
    "    LEFT JOIN reviews r ON p.id = r.product_id\n",
    "    GROUP BY p.id, p.name, c.name, p.brand, p.price\n",
    "    HAVING SUM(oi.line_total) IS NOT NULL\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "top_products.show(truncate=False)\n",
    "\n",
    "# Category performance analysis\n",
    "category_performance = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.name as category_name,\n",
    "        COUNT(DISTINCT p.id) as product_count,\n",
    "        COUNT(oi.id) as total_orders,\n",
    "        SUM(oi.quantity) as units_sold,\n",
    "        SUM(oi.line_total) as category_revenue,\n",
    "        AVG(oi.unit_price) as avg_price,\n",
    "        AVG(r.rating) as avg_rating\n",
    "    FROM categories c\n",
    "    LEFT JOIN products p ON c.id = p.category_id\n",
    "    LEFT JOIN order_items oi ON p.id = oi.product_id\n",
    "    LEFT JOIN orders o ON oi.order_id = o.id AND o.payment_status = 'paid'\n",
    "    LEFT JOIN reviews r ON p.id = r.product_id\n",
    "    WHERE c.parent_category_id IS NULL  -- Only top-level categories\n",
    "    GROUP BY c.id, c.name\n",
    "    HAVING SUM(oi.line_total) IS NOT NULL\n",
    "    ORDER BY category_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "category_performance.show()\n",
    "\n",
    "# Visualize category performance\n",
    "cat_pandas = category_performance.toPandas()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(range(len(cat_pandas)), cat_pandas['category_revenue'])\n",
    "plt.title('Revenue by Category')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.xticks(range(len(cat_pandas)), cat_pandas['category_name'], rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(range(len(cat_pandas)), cat_pandas['units_sold'])\n",
    "plt.title('Units Sold by Category')\n",
    "plt.ylabel('Units Sold')\n",
    "plt.xticks(range(len(cat_pandas)), cat_pandas['category_name'], rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(range(len(cat_pandas)), cat_pandas['avg_rating'])\n",
    "plt.title('Average Rating by Category')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.xticks(range(len(cat_pandas)), cat_pandas['category_name'], rotation=45)\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# EXAMPLE 4: CUSTOMER SEGMENTATION WITH SPARK ML\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 4: CUSTOMER SEGMENTATION USING RFM ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# RFM Analysis (Recency, Frequency, Monetary)\n",
    "# First, let's create RFM features for each customer\n",
    "current_date = lit(\"2025-01-01\").cast(\"date\")\n",
    "\n",
    "rfm_data = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.id as customer_id,\n",
    "        c.first_name || ' ' || c.last_name as customer_name,\n",
    "        c.customer_tier,\n",
    "        DATEDIFF(CURRENT_DATE(), MAX(o.order_date)) as recency_days,\n",
    "        COUNT(o.id) as frequency,\n",
    "        SUM(o.total_amount) as monetary_value,\n",
    "        AVG(o.total_amount) as avg_order_value\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.id = o.customer_id AND o.payment_status = 'paid'\n",
    "    WHERE o.id IS NOT NULL  -- Only customers with orders\n",
    "    GROUP BY c.id, c.first_name, c.last_name, c.customer_tier\n",
    "    HAVING COUNT(o.id) > 0\n",
    "\"\"\")\n",
    "\n",
    "rfm_data.show(20)\n",
    "\n",
    "# Prepare features for clustering\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"recency_days\", \"frequency\", \"monetary_value\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "rfm_features = assembler.transform(rfm_data)\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(k=4, seed=42, featuresCol=\"features\", predictionCol=\"segment\")\n",
    "model = kmeans.fit(rfm_features)\n",
    "segmented_customers = model.transform(rfm_features)\n",
    "\n",
    "# Analyze segments\n",
    "segment_analysis = segmented_customers.groupBy(\"segment\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"customer_count\"),\n",
    "        avg(\"recency_days\").alias(\"avg_recency\"),\n",
    "        avg(\"frequency\").alias(\"avg_frequency\"),\n",
    "        avg(\"monetary_value\").alias(\"avg_monetary\"),\n",
    "        avg(\"avg_order_value\").alias(\"avg_order_value\")\n",
    "    ) \\\n",
    "    .orderBy(\"segment\")\n",
    "\n",
    "print(\"Customer Segments Analysis:\")\n",
    "segment_analysis.show()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# EXAMPLE 5: REAL-TIME STREAMING SIMULATION\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 5: ADVANCED WINDOW FUNCTIONS & ANALYTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Customer purchase patterns using window functions\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Analyze customer purchasing patterns\n",
    "customer_window = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "customer_patterns = orders_df.filter(col(\"payment_status\") == \"paid\") \\\n",
    "    .withColumn(\"order_number\", row_number().over(customer_window)) \\\n",
    "    .withColumn(\"prev_order_date\", lag(\"order_date\").over(customer_window)) \\\n",
    "    .withColumn(\"days_between_orders\", \n",
    "                datediff(col(\"order_date\"), col(\"prev_order_date\"))) \\\n",
    "    .withColumn(\"running_total\", \n",
    "                sum(\"total_amount\").over(customer_window.rowsBetween(Window.unboundedPreceding, Window.currentRow)))\n",
    "\n",
    "# Customer loyalty analysis\n",
    "loyalty_analysis = customer_patterns.filter(col(\"order_number\") > 1) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        max(\"order_number\").alias(\"total_orders\"),\n",
    "        avg(\"days_between_orders\").alias(\"avg_days_between_orders\"),\n",
    "        max(\"running_total\").alias(\"lifetime_value\"),\n",
    "        stddev(\"total_amount\").alias(\"order_value_consistency\")\n",
    "    ) \\\n",
    "    .filter(col(\"total_orders\") >= 3)  # Customers with 3+ orders\n",
    "\n",
    "print(\"Customer Loyalty Metrics:\")\n",
    "loyalty_analysis.show(20)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# EXAMPLE 6: INVENTORY & STOCK ANALYSIS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 6: INVENTORY OPTIMIZATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Products that need restocking\n",
    "low_stock_products = products_df.filter(col(\"stock_quantity\") <= col(\"reorder_level\")) \\\n",
    "    .select(\"sku\", \"name\", \"stock_quantity\", \"reorder_level\", \"category_id\") \\\n",
    "    .join(categories_df, products_df.category_id == categories_df.id) \\\n",
    "    .select(\"sku\", \"name\", \"stock_quantity\", \"reorder_level\", categories_df.name.alias(\"category\"))\n",
    "\n",
    "print(\"Products Needing Restock:\")\n",
    "low_stock_products.show(truncate=False)\n",
    "\n",
    "# Sales velocity analysis\n",
    "sales_velocity = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.sku,\n",
    "        p.name as product_name,\n",
    "        p.stock_quantity,\n",
    "        p.reorder_level,\n",
    "        SUM(oi.quantity) as total_sold_90days,\n",
    "        SUM(oi.quantity) / 90.0 as daily_sales_rate,\n",
    "        p.stock_quantity / (SUM(oi.quantity) / 90.0) as days_of_inventory\n",
    "    FROM products p\n",
    "    LEFT JOIN order_items oi ON p.id = oi.product_id\n",
    "    LEFT JOIN orders o ON oi.order_id = o.id \n",
    "    WHERE o.payment_status = 'paid' \n",
    "    AND o.order_date >= DATE_SUB(CURRENT_DATE(), 90)\n",
    "    GROUP BY p.id, p.sku, p.name, p.stock_quantity, p.reorder_level\n",
    "    HAVING SUM(oi.quantity) > 0\n",
    "    ORDER BY days_of_inventory ASC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Sales Velocity Analysis (Products with lowest days of inventory):\")\n",
    "sales_velocity.show(20)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# CLEANUP\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key insights generated:\")\n",
    "print(\"✅ Customer LTV analysis by tier\")\n",
    "print(\"✅ Monthly sales trends and patterns\")\n",
    "print(\"✅ Product and category performance\")\n",
    "print(\"✅ Customer segmentation using RFM analysis\")\n",
    "print(\"✅ Customer loyalty and purchasing patterns\")\n",
    "print(\"✅ Inventory optimization recommendations\")\n",
    "print(\"\\nYou can now explore the data further or create your own analyses!\")\n",
    "\n",
    "# Don't stop the Spark session - leave it running for interactive use\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
