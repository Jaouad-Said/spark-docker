{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e0873a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Spark + PostgreSQL Analytics Examples\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Save this as: spark/notebooks/spark_analytics_examples.ipynb\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# SETUP & CONNECTION\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Spark + PostgreSQL Analytics Examples\n",
    "# Save this as: spark/notebooks/spark_analytics_examples.ipynb\n",
    "\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "# SETUP & CONNECTION\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark with PostgreSQL connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-Commerce Analytics\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.7.5.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# PostgreSQL connection properties\n",
    "pg_properties = {\n",
    "    \"user\": \"sparkuser\",\n",
    "    \"password\": \"sparkpass\", \n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"url\": \"jdbc:postgresql://postgres:5432/demo\"\n",
    "}\n",
    "\n",
    "print(\"笨 Spark session created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "# DATA LOADING\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "\n",
    "# Load main tables into Spark DataFrames\n",
    "customers_df = spark.read.jdbc(**pg_properties, table=\"customers\")\n",
    "orders_df = spark.read.jdbc(**pg_properties, table=\"orders\")\n",
    "order_items_df = spark.read.jdbc(**pg_properties, table=\"order_items\")\n",
    "products_df = spark.read.jdbc(**pg_properties, table=\"products\")\n",
    "categories_df = spark.read.jdbc(**pg_properties, table=\"categories\")\n",
    "reviews_df = spark.read.jdbc(**pg_properties, table=\"reviews\")\n",
    "\n",
    "print(\"沒 Data loaded successfully!\")\n",
    "print(f\"Customers: {customers_df.count():,}\")\n",
    "print(f\"Orders: {orders_df.count():,}\")\n",
    "print(f\"Order Items: {order_items_df.count():,}\")\n",
    "print(f\"Products: {products_df.count():,}\")\n",
    "print(f\"Reviews: {reviews_df.count():,}\")\n",
    "\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "# EXAMPLE 1: CUSTOMER ANALYTICS\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 1: CUSTOMER LIFETIME VALUE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Customer LTV analysis with Spark SQL\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "customer_ltv = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_tier,\n",
    "        COUNT(DISTINCT c.id) as customer_count,\n",
    "        AVG(c.total_lifetime_value) as avg_ltv,\n",
    "        SUM(c.total_lifetime_value) as total_ltv,\n",
    "        AVG(DATEDIFF(CURRENT_DATE(), c.registration_date)) as avg_days_since_registration\n",
    "    FROM customers c\n",
    "    WHERE c.total_lifetime_value > 0\n",
    "    GROUP BY c.customer_tier\n",
    "    ORDER BY avg_ltv DESC\n",
    "\"\"\")\n",
    "\n",
    "customer_ltv.show()\n",
    "\n",
    "# Convert to pandas for visualization\n",
    "ltv_pandas = customer_ltv.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(ltv_pandas['customer_tier'], ltv_pandas['avg_ltv'])\n",
    "plt.title('Average LTV by Customer Tier')\n",
    "plt.ylabel('Average LTV ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(ltv_pandas['customer_count'], labels=ltv_pandas['customer_tier'], autopct='%1.1f%%')\n",
    "plt.title('Customer Distribution by Tier')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "# EXAMPLE 2: SALES TREND ANALYSIS\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 2: MONTHLY SALES TREND ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Monthly sales trends\n",
    "monthly_sales = orders_df.filter(col(\"payment_status\") == \"paid\") \\\n",
    "    .withColumn(\"year_month\", date_format(col(\"order_date\"), \"yyyy-MM\")) \\\n",
    "    .groupBy(\"year_month\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_orders\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "    ) \\\n",
    "    .orderBy(\"year_month\")\n",
    "\n",
    "monthly_sales.show(20)\n",
    "\n",
    "# Plot sales trends\n",
    "sales_pandas = monthly_sales.toPandas()\n",
    "sales_pandas['year_month'] = pd.to_datetime(sales_pandas['year_month'])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(sales_pandas['year_month'], sales_pandas['total_revenue'])\n",
    "plt.title('Monthly Revenue Trend')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(sales_pandas['year_month'], sales_pandas['total_orders'])\n",
    "plt.title('Monthly Order Volume')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(sales_pandas['year_month'], sales_pandas['avg_order_value'])\n",
    "plt.title('Average Order Value Trend')\n",
    "plt.ylabel('AOV ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(sales_pandas['year_month'], sales_pandas['unique_customers'])\n",
    "plt.title('Monthly Unique Customers')\n",
    "plt.ylabel('Unique Customers')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "# EXAMPLE 3: PRODUCT PERFORMANCE ANALYSIS\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 3: TOP PERFORMING PRODUCTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create temp views for complex analysis\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "order_items_df.createOrReplaceTempView(\"order_items\")\n",
    "categories_df.createOrReplaceTempView(\"categories\")\n",
    "reviews_df.createOrReplaceTempView(\"reviews\")\n",
    "\n",
    "# Top products by revenue with category information\n",
    "top_products = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.name as product_name,\n",
    "        c.name as category_name,\n",
    "        p.brand,\n",
    "        p.price,\n",
    "        COUNT(oi.id) as total_orders,\n",
    "        SUM(oi.quantity) as units_sold,\n",
    "        SUM(oi.line_total) as total_revenue,\n",
    "        AVG(r.rating) as avg_rating,\n",
    "        COUNT(r.id) as review_count\n",
    "    FROM products p\n",
    "    LEFT JOIN categories c ON p.category_id = c.id\n",
    "    LEFT JOIN order_items oi ON p.id = oi.product_id\n",
    "    LEFT JOIN orders o ON oi.order_id = o.id AND o.payment_status = 'paid'\n",
    "    LEFT JOIN reviews r ON p.id = r.product_id\n",
    "    GROUP BY p.id, p.name, c.name, p.brand, p.price\n",
    "    HAVING SUM(oi.line_total) IS NOT NULL\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "top_products.show(truncate=False)\n",
    "\n",
    "# Category performance analysis\n",
    "category_performance = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.name as category_name,\n",
    "        COUNT(DISTINCT p.id) as product_count,\n",
    "        COUNT(oi.id) as total_orders,\n",
    "        SUM(oi.quantity) as units_sold,\n",
    "        SUM(oi.line_total) as category_revenue,\n",
    "        AVG(oi.unit_price) as avg_price,\n",
    "        AVG(r.rating) as avg_rating\n",
    "    FROM categories c\n",
    "    LEFT JOIN products p ON c.id = p.category_id\n",
    "    LEFT JOIN order_items oi ON p.id = oi.product_id\n",
    "    LEFT JOIN orders o ON oi.order_id = o.id AND o.payment_status = 'paid'\n",
    "    LEFT JOIN reviews r ON p.id = r.product_id\n",
    "    WHERE c.parent_category_id IS NULL  -- Only top-level categories\n",
    "    GROUP BY c.id, c.name\n",
    "    HAVING SUM(oi.line_total) IS NOT NULL\n",
    "    ORDER BY category_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "category_performance.show()\n",
    "\n",
    "# Visualize category performance\n",
    "cat_pandas = category_performance.toPandas()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(range(len(cat_pandas)), cat_pandas['category_revenue'])\n",
    "plt.title('Revenue by Category')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.xticks(range(len(cat_pandas)), cat_pandas['category_name'], rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(range(len(cat_pandas)), cat_pandas['units_sold'])\n",
    "plt.title('Units Sold by Category')\n",
    "plt.ylabel('Units Sold')\n",
    "plt.xticks(range(len(cat_pandas)), cat_pandas['category_name'], rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(range(len(cat_pandas)), cat_pandas['avg_rating'])\n",
    "plt.title('Average Rating by Category')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.xticks(range(len(cat_pandas)), cat_pandas['category_name'], rotation=45)\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "# EXAMPLE 4: CUSTOMER SEGMENTATION WITH SPARK ML\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 4: CUSTOMER SEGMENTATION USING RFM ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# RFM Analysis (Recency, Frequency, Monetary)\n",
    "# First, let's create RFM features for each customer\n",
    "current_date = lit(\"2025-01-01\").cast(\"date\")\n",
    "\n",
    "rfm_data = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.id as customer_id,\n",
    "        c.first_name || ' ' || c.last_name as customer_name,\n",
    "        c.customer_tier,\n",
    "        DATEDIFF(CURRENT_DATE(), MAX(o.order_date)) as recency_days,\n",
    "        COUNT(o.id) as frequency,\n",
    "        SUM(o.total_amount) as monetary_value,\n",
    "        AVG(o.total_amount) as avg_order_value\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.id = o.customer_id AND o.payment_status = 'paid'\n",
    "    WHERE o.id IS NOT NULL  -- Only customers with orders\n",
    "    GROUP BY c.id, c.first_name, c.last_name, c.customer_tier\n",
    "    HAVING COUNT(o.id) > 0\n",
    "\"\"\")\n",
    "\n",
    "rfm_data.show(20)\n",
    "\n",
    "# Prepare features for clustering\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"recency_days\", \"frequency\", \"monetary_value\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "rfm_features = assembler.transform(rfm_data)\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(k=4, seed=42, featuresCol=\"features\", predictionCol=\"segment\")\n",
    "model = kmeans.fit(rfm_features)\n",
    "segmented_customers = model.transform(rfm_features)\n",
    "\n",
    "# Analyze segments\n",
    "segment_analysis = segmented_customers.groupBy(\"segment\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"customer_count\"),\n",
    "        avg(\"recency_days\").alias(\"avg_recency\"),\n",
    "        avg(\"frequency\").alias(\"avg_frequency\"),\n",
    "        avg(\"monetary_value\").alias(\"avg_monetary\"),\n",
    "        avg(\"avg_order_value\").alias(\"avg_order_value\")\n",
    "    ) \\\n",
    "    .orderBy(\"segment\")\n",
    "\n",
    "print(\"Customer Segments Analysis:\")\n",
    "segment_analysis.show()\n",
    "\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "# EXAMPLE 5: REAL-TIME STREAMING SIMULATION\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 5: ADVANCED WINDOW FUNCTIONS & ANALYTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Customer purchase patterns using window functions\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Analyze customer purchasing patterns\n",
    "customer_window = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "customer_patterns = orders_df.filter(col(\"payment_status\") == \"paid\") \\\n",
    "    .withColumn(\"order_number\", row_number().over(customer_window)) \\\n",
    "    .withColumn(\"prev_order_date\", lag(\"order_date\").over(customer_window)) \\\n",
    "    .withColumn(\"days_between_orders\", \n",
    "                datediff(col(\"order_date\"), col(\"prev_order_date\"))) \\\n",
    "    .withColumn(\"running_total\", \n",
    "                sum(\"total_amount\").over(customer_window.rowsBetween(Window.unboundedPreceding, Window.currentRow)))\n",
    "\n",
    "# Customer loyalty analysis\n",
    "loyalty_analysis = customer_patterns.filter(col(\"order_number\") > 1) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        max(\"order_number\").alias(\"total_orders\"),\n",
    "        avg(\"days_between_orders\").alias(\"avg_days_between_orders\"),\n",
    "        max(\"running_total\").alias(\"lifetime_value\"),\n",
    "        stddev(\"total_amount\").alias(\"order_value_consistency\")\n",
    "    ) \\\n",
    "    .filter(col(\"total_orders\") >= 3)  # Customers with 3+ orders\n",
    "\n",
    "print(\"Customer Loyalty Metrics:\")\n",
    "loyalty_analysis.show(20)\n",
    "\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "# EXAMPLE 6: INVENTORY & STOCK ANALYSIS\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE 6: INVENTORY OPTIMIZATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Products that need restocking\n",
    "low_stock_products = products_df.filter(col(\"stock_quantity\") <= col(\"reorder_level\")) \\\n",
    "    .select(\"sku\", \"name\", \"stock_quantity\", \"reorder_level\", \"category_id\") \\\n",
    "    .join(categories_df, products_df.category_id == categories_df.id) \\\n",
    "    .select(\"sku\", \"name\", \"stock_quantity\", \"reorder_level\", categories_df.name.alias(\"category\"))\n",
    "\n",
    "print(\"Products Needing Restock:\")\n",
    "low_stock_products.show(truncate=False)\n",
    "\n",
    "# Sales velocity analysis\n",
    "sales_velocity = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.sku,\n",
    "        p.name as product_name,\n",
    "        p.stock_quantity,\n",
    "        p.reorder_level,\n",
    "        SUM(oi.quantity) as total_sold_90days,\n",
    "        SUM(oi.quantity) / 90.0 as daily_sales_rate,\n",
    "        p.stock_quantity / (SUM(oi.quantity) / 90.0) as days_of_inventory\n",
    "    FROM products p\n",
    "    LEFT JOIN order_items oi ON p.id = oi.product_id\n",
    "    LEFT JOIN orders o ON oi.order_id = o.id \n",
    "    WHERE o.payment_status = 'paid' \n",
    "    AND o.order_date >= DATE_SUB(CURRENT_DATE(), 90)\n",
    "    GROUP BY p.id, p.sku, p.name, p.stock_quantity, p.reorder_level\n",
    "    HAVING SUM(oi.quantity) > 0\n",
    "    ORDER BY days_of_inventory ASC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Sales Velocity Analysis (Products with lowest days of inventory):\")\n",
    "sales_velocity.show(20)\n",
    "\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "# CLEANUP\n",
    "# 笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"沁 ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key insights generated:\")\n",
    "print(\"笨 Customer LTV analysis by tier\")\n",
    "print(\"笨 Monthly sales trends and patterns\")\n",
    "print(\"笨 Product and category performance\")\n",
    "print(\"笨 Customer segmentation using RFM analysis\")\n",
    "print(\"笨 Customer loyalty and purchasing patterns\")\n",
    "print(\"笨 Inventory optimization recommendations\")\n",
    "print(\"\\nYou can now explore the data further or create your own analyses!\")\n",
    "\n",
    "# Don't stop the Spark session - leave it running for interactive use\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
